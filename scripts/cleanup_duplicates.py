#!/usr/bin/env python3
"""
é‡è¤‡ã‚¤ãƒ³ãƒãƒ¼ãƒˆã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

WorksPhotesãƒ•ã‚©ãƒ«ãƒ€ã‹ã‚‰å¤šé‡å–ã‚Šè¾¼ã¿ã•ã‚ŒãŸä»¥ä¸‹ã‚’å‰Šé™¤ã™ã‚‹:
1. æ•´å‚™æ¸ˆ=OFF ã® Notion ãƒšãƒ¼ã‚¸ â†’ ã‚¢ãƒ¼ã‚«ã‚¤ãƒ– + ç´ã¥ãR2ç”»åƒã‚’å‰Šé™¤
2. Notionãƒšãƒ¼ã‚¸ã«ç´ã¥ã‹ãªã„å­¤ç«‹R2ç”»åƒ â†’ ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ã§ï¼‰å‰Šé™¤

ã‚®ãƒ£ãƒ©ãƒªãƒ¼ã«åæ˜ æ¸ˆã¿ï¼ˆæ•´å‚™æ¸ˆ=ONï¼‰ã®ã‚¢ã‚¤ãƒ†ãƒ ã¯ä¸€åˆ‡è§¦ã‚Œãªã„ã€‚
æœªæ•´ç†ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ç”»åƒãŒã‚ã‚‹é‹ç”¨ã‚’è€ƒæ…®ã—ã€å­¤ç«‹R2ç”»åƒã®å‰Šé™¤ã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆç„¡åŠ¹ã€‚

ä½¿ã„æ–¹:
  # dry-runï¼ˆç¢ºèªã®ã¿ï¼‰
  python scripts/cleanup_duplicates.py

  # å­¤ç«‹R2ç”»åƒã‚‚å‰Šé™¤å¯¾è±¡ã«å«ã‚ã‚‹ï¼ˆæ˜ç¤ºæŒ‡å®šï¼‰
  python scripts/cleanup_duplicates.py --delete-orphaned-r2

  # å®Ÿè¡Œ
  python scripts/cleanup_duplicates.py --execute
"""

import argparse
import logging
import sys
from pathlib import Path
from urllib.parse import urlparse

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
ROOT_DIR = Path(__file__).resolve().parent.parent
sys.path.insert(0, str(ROOT_DIR / "src"))

from auto_post.config import Config  # noqa: E402
from auto_post.notion_db import NotionDB  # noqa: E402
from auto_post.r2_storage import R2Storage  # noqa: E402

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    handlers=[logging.StreamHandler(sys.stdout)],
)
# Reduce noise from libraries
logging.getLogger("httpx").setLevel(logging.WARNING)
logging.getLogger("httpcore").setLevel(logging.WARNING)
logging.getLogger("botocore").setLevel(logging.WARNING)
logging.getLogger("urllib3").setLevel(logging.WARNING)

logger = logging.getLogger(__name__)

# ---------- æ•´å‚™æ¸ˆãƒ—ãƒ­ãƒ‘ãƒ†ã‚£åå€™è£œ ----------
READY_PROP_CANDIDATES = ("æ•´å‚™æ¸ˆã¿", "æ•´å‚™æ¸ˆ")
KNOWN_R2_PREFIXES = (
    "photos/",
    "photos-light/",
    "images/",
    "images-light/",
    "uploads/",
    "uploads-light/",
)


def _resolve_ready_prop(db_info: dict) -> str:
    """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚¹ã‚­ãƒ¼ãƒã‹ã‚‰æ•´å‚™æ¸ˆãƒ—ãƒ­ãƒ‘ãƒ†ã‚£åã‚’è§£æ±ºã™ã‚‹ã€‚"""
    props = db_info.get("properties", {})
    for name in READY_PROP_CANDIDATES:
        schema = props.get(name)
        if isinstance(schema, dict) and schema.get("type") in {"checkbox", "formula"}:
            return name
    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: checkbox/formula ã§ã€Œæ•´å‚™ã€ã‚’å«ã‚€ãƒ—ãƒ­ãƒ‘ãƒ†ã‚£
    for name, schema in props.items():
        if not isinstance(schema, dict):
            continue
        if schema.get("type") not in {"checkbox", "formula"}:
            continue
        if "æ•´å‚™" in str(name).lower() or "ready" in str(name).lower():
            return name
    raise ValueError("æ•´å‚™æ¸ˆãƒ—ãƒ­ãƒ‘ãƒ†ã‚£ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")


def _is_page_ready(page: dict, ready_prop: str) -> bool:
    """ãƒšãƒ¼ã‚¸ãŒæ•´å‚™æ¸ˆã‹ã©ã†ã‹åˆ¤å®šã™ã‚‹ã€‚"""
    prop = page.get("properties", {}).get(ready_prop, {})
    prop_type = prop.get("type")
    if prop_type == "checkbox":
        return bool(prop.get("checkbox"))
    if prop_type == "formula":
        formula = prop.get("formula") or {}
        if formula.get("type") == "boolean":
            return bool(formula.get("boolean"))
    return False


def _get_page_title(page: dict) -> str:
    """ãƒšãƒ¼ã‚¸ã®ä½œå“åã‚’å–å¾—ã™ã‚‹ã€‚"""
    title_prop = page.get("properties", {}).get("ä½œå“å", {})
    titles = title_prop.get("title", [])
    if titles:
        return titles[0].get("plain_text", "(ç„¡é¡Œ)")
    return "(ç„¡é¡Œ)"


def _get_page_image_urls(page: dict) -> list[str]:
    """ãƒšãƒ¼ã‚¸ã‹ã‚‰ç”»åƒURLã‚’æŠ½å‡ºã™ã‚‹ã€‚"""
    files_prop = page.get("properties", {}).get("ç”»åƒ", {})
    files = files_prop.get("files", [])
    urls = []
    for f in files:
        if f.get("type") == "external":
            url = f.get("external", {}).get("url", "")
            if url:
                urls.append(url)
        elif f.get("type") == "file":
            url = f.get("file", {}).get("url", "")
            if url:
                urls.append(url)
    return urls


def _url_to_r2_key(url: str, public_url: str) -> str | None:
    """ç”»åƒURLã‹ã‚‰R2ã‚­ãƒ¼ã‚’æŠ½å‡ºã™ã‚‹ã€‚"""
    if not url:
        return None
    # https://pub-xxx.r2.dev/photos/xxx_yyy.jpg -> photos/xxx_yyy.jpg
    if public_url and url.startswith(public_url):
        key = url[len(public_url) :].lstrip("/")
        return key if key else None
    # URLãƒ‘ãƒ¼ã‚¹ã§ãƒ‘ã‚¹ã ã‘å–ã‚‹
    parsed = urlparse(url)
    path = parsed.path.lstrip("/")
    if path.startswith(KNOWN_R2_PREFIXES):
        return path
    return None


def _list_r2_keys_with_prefix(r2: R2Storage, prefix: str) -> list[str]:
    """æŒ‡å®šprefixé…ä¸‹ã®R2ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚­ãƒ¼ã‚’ãƒªã‚¹ãƒˆã™ã‚‹ã€‚"""
    keys: list[str] = []
    client = r2._create_client()
    continuation_token = None

    while True:
        kwargs = {
            "Bucket": r2.config.bucket_name,
            "Prefix": prefix,
            "MaxKeys": 1000,
        }
        if continuation_token:
            kwargs["ContinuationToken"] = continuation_token

        response = client.list_objects_v2(**kwargs)
        contents = response.get("Contents", [])
        for obj in contents:
            key = obj.get("Key", "")
            if key:
                keys.append(key)

        if response.get("IsTruncated"):
            continuation_token = response.get("NextContinuationToken")
        else:
            break

    return keys


def _list_all_r2_keys(
    r2: R2Storage,
    prefixes: tuple[str, ...] = KNOWN_R2_PREFIXES,
) -> set[str]:
    """R2ãƒã‚±ãƒƒãƒˆå†…ã®æ—¢çŸ¥ç”»åƒprefixé…ä¸‹ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚­ãƒ¼ã‚’ãƒªã‚¹ãƒˆã™ã‚‹ã€‚"""
    all_keys: set[str] = set()
    for prefix in prefixes:
        all_keys.update(_list_r2_keys_with_prefix(r2, prefix=prefix))
    return all_keys


def main():
    parser = argparse.ArgumentParser(description="é‡è¤‡ã‚¤ãƒ³ãƒãƒ¼ãƒˆãƒ‡ãƒ¼ã‚¿ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—")
    parser.add_argument(
        "--execute",
        action="store_true",
        help="å®Ÿéš›ã«å‰Šé™¤ã‚’å®Ÿè¡Œã™ã‚‹ (æŒ‡å®šã—ãªã„å ´åˆã¯dry-run)",
    )
    parser.add_argument(
        "--env-file",
        type=Path,
        default=ROOT_DIR / ".env",
        help=".envãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹",
    )
    parser.add_argument(
        "--delete-orphaned-r2",
        action="store_true",
        help="Notionã«æœªç´ã¥ã‘ã®å­¤ç«‹R2ç”»åƒã‚‚å‰Šé™¤å¯¾è±¡ã«å«ã‚ã‚‹ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: å«ã‚ãªã„ï¼‰",
    )
    args = parser.parse_args()

    dry_run = not args.execute
    delete_orphaned_r2 = bool(args.delete_orphaned_r2)

    if dry_run:
        print("=" * 60)
        print("ğŸ” DRY-RUN ãƒ¢ãƒ¼ãƒ‰ (å‰Šé™¤ã¯å®Ÿè¡Œã•ã‚Œã¾ã›ã‚“)")
        print("=" * 60)
    else:
        print("=" * 60)
        print("âš ï¸  EXECUTE ãƒ¢ãƒ¼ãƒ‰ (å®Ÿéš›ã«ãƒ‡ãƒ¼ã‚¿ã‚’å‰Šé™¤ã—ã¾ã™)")
        print("=" * 60)
        confirm = input("æœ¬å½“ã«å®Ÿè¡Œã—ã¾ã™ã‹ï¼Ÿ (yes/no): ")
        if confirm.strip().lower() != "yes":
            print("ã‚­ãƒ£ãƒ³ã‚»ãƒ«ã—ã¾ã—ãŸã€‚")
            return

    # è¨­å®šèª­ã¿è¾¼ã¿
    config = Config.load(env_file=args.env_file, allow_missing_instagram=True)
    notion = NotionDB(
        config.notion.token,
        config.notion.database_id,
        config.notion.tags_database_id,
    )
    r2 = R2Storage(config.r2)
    public_url = config.r2.public_url or ""

    # ------------------------------------------------
    # Step 1: Notion å…¨ãƒšãƒ¼ã‚¸å–å¾—
    # ------------------------------------------------
    print("\nğŸ“‹ Notionãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‹ã‚‰å…¨ãƒšãƒ¼ã‚¸ã‚’å–å¾—ä¸­...")
    db_info = notion.get_database_info()
    ready_prop = _resolve_ready_prop(db_info)
    print(f"  æ•´å‚™æ¸ˆãƒ—ãƒ­ãƒ‘ãƒ†ã‚£: {ready_prop}")

    all_pages = notion.list_database_pages(notion.database_id)
    print(f"  ç·ãƒšãƒ¼ã‚¸æ•°: {len(all_pages)}")

    # åˆ†é¡
    ready_pages = []
    not_ready_pages = []
    notion_r2_keys_all = set()  # å…¨ãƒšãƒ¼ã‚¸ã‹ã‚‰å‚ç…§ã•ã‚Œã¦ã„ã‚‹R2ã‚­ãƒ¼
    notion_r2_keys_ready = set()  # æ•´å‚™æ¸ˆãƒšãƒ¼ã‚¸ã‹ã‚‰å‚ç…§ã•ã‚Œã¦ã„ã‚‹R2ã‚­ãƒ¼

    for page in all_pages:
        is_ready = _is_page_ready(page, ready_prop)
        image_urls = _get_page_image_urls(page)
        r2_keys = set()
        for url in image_urls:
            key = _url_to_r2_key(url, public_url)
            if key:
                r2_keys.add(key)

        notion_r2_keys_all.update(r2_keys)

        if is_ready:
            ready_pages.append(page)
            notion_r2_keys_ready.update(r2_keys)
        else:
            not_ready_pages.append(page)

    print(f"  æ•´å‚™æ¸ˆ: {len(ready_pages)} ä»¶ (ä¿æŒ)")
    print(f"  æœªæ•´å‚™: {len(not_ready_pages)} ä»¶ (å‰Šé™¤å€™è£œ)")

    # ------------------------------------------------
    # Step 2: R2 å…¨ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆå–å¾—
    # ------------------------------------------------
    print("\nğŸ“¦ R2ãƒã‚±ãƒƒãƒˆã‹ã‚‰å…¨ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚­ãƒ¼ã‚’å–å¾—ä¸­...")
    all_r2_keys = _list_all_r2_keys(r2)
    joined_prefixes = ", ".join(KNOWN_R2_PREFIXES)
    print(f"  R2å¯¾è±¡prefix: {joined_prefixes}")
    print(f"  R2å¯¾è±¡ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆç·æ•°: {len(all_r2_keys)}")

    # å­¤ç«‹R2ã‚­ãƒ¼ = R2ã«ã‚ã‚‹ãŒã€Notionã®ã©ã®ãƒšãƒ¼ã‚¸ã‹ã‚‰ã‚‚å‚ç…§ã•ã‚Œã¦ã„ãªã„ã‚‚ã®
    orphaned_r2_keys = all_r2_keys - notion_r2_keys_all
    print(f"  Notionã‹ã‚‰å‚ç…§ã•ã‚Œã¦ã„ãªã„å­¤ç«‹ç”»åƒ: {len(orphaned_r2_keys)} ä»¶")

    # æœªæ•´å‚™ãƒšãƒ¼ã‚¸ã®R2ã‚­ãƒ¼ï¼ˆæ•´å‚™æ¸ˆãƒšãƒ¼ã‚¸ã‹ã‚‰ã‚‚å‚ç…§ã•ã‚Œã¦ã„ã‚‹ã‚‚ã®ã¯é™¤å¤–ï¼‰
    not_ready_r2_keys = set()
    for page in not_ready_pages:
        image_urls = _get_page_image_urls(page)
        for url in image_urls:
            key = _url_to_r2_key(url, public_url)
            if key and key not in notion_r2_keys_ready:
                not_ready_r2_keys.add(key)

    # ------------------------------------------------
    # Step 3: å‰Šé™¤å¯¾è±¡ã®ã‚µãƒãƒªãƒ¼
    # ------------------------------------------------
    # å‰Šé™¤å¯¾è±¡R2ã‚­ãƒ¼
    # - default: æœªæ•´å‚™ãƒšãƒ¼ã‚¸ç”±æ¥ã®ã¿
    # - --delete-orphaned-r2 æŒ‡å®šæ™‚: å­¤ç«‹ã‚­ãƒ¼ã‚‚å«ã‚ã‚‹
    r2_keys_to_delete = set(not_ready_r2_keys)
    if delete_orphaned_r2:
        r2_keys_to_delete.update(orphaned_r2_keys)
    notion_pages_to_archive = not_ready_pages

    print("\n" + "=" * 60)
    print("ğŸ“Š ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å¯¾è±¡ã‚µãƒãƒªãƒ¼")
    print("=" * 60)
    print(f"\nğŸ—‚  Notionãƒšãƒ¼ã‚¸ ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–å¯¾è±¡: {len(notion_pages_to_archive)} ä»¶")
    for page in notion_pages_to_archive:
        title = _get_page_title(page)
        page_id = page["id"]
        image_urls = _get_page_image_urls(page)
        print(f"  - [{page_id[:8]}...] {title} (ç”»åƒ: {len(image_urls)}æš)")

    print(f"\nğŸ–¼  R2ç”»åƒ å‰Šé™¤å¯¾è±¡: {len(r2_keys_to_delete)} ä»¶")
    print("    å†…è¨³:")
    if delete_orphaned_r2:
        print(f"      å­¤ç«‹ç”»åƒ (Notionãƒšãƒ¼ã‚¸ãªã—): {len(orphaned_r2_keys)} ä»¶")
    else:
        print(f"      å­¤ç«‹ç”»åƒ (Notionãƒšãƒ¼ã‚¸ãªã—): {len(orphaned_r2_keys)} ä»¶ï¼ˆä»Šå›ã¯å‰Šé™¤å¯¾è±¡å¤–ï¼‰")
    print(f"      æœªæ•´å‚™ãƒšãƒ¼ã‚¸ã®ç”»åƒ: {len(not_ready_r2_keys)} ä»¶")

    if len(r2_keys_to_delete) <= 50:
        for key in sorted(r2_keys_to_delete):
            tag = "å­¤ç«‹" if key in orphaned_r2_keys else "æœªæ•´å‚™"
            print(f"    - [{tag}] {key}")
    else:
        print("    (ä»¶æ•°ãŒå¤šã„ãŸã‚å…ˆé ­20ä»¶ã‚’è¡¨ç¤º)")
        for key in sorted(r2_keys_to_delete)[:20]:
            tag = "å­¤ç«‹" if key in orphaned_r2_keys else "æœªæ•´å‚™"
            print(f"    - [{tag}] {key}")
        print(f"    ... ä»– {len(r2_keys_to_delete) - 20} ä»¶")

    print(f"\nâœ… ä¿æŒ: æ•´å‚™æ¸ˆ {len(ready_pages)} ä»¶ã®Notionãƒšãƒ¼ã‚¸ã¨ç´ã¥ãR2ç”»åƒ")

    # ------------------------------------------------
    # Step 4: å®Ÿè¡Œ
    # ------------------------------------------------
    if dry_run:
        print("\n" + "=" * 60)
        print(
            "ğŸ” DRY-RUN å®Œäº†ã€‚å®Ÿéš›ã«å‰Šé™¤ã™ã‚‹ã«ã¯ --execute ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã‚’ä»˜ã‘ã¦å†å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚"
        )
        if not delete_orphaned_r2 and orphaned_r2_keys:
            print("   å­¤ç«‹R2ç”»åƒã‚‚å‰Šé™¤ã—ãŸã„å ´åˆã¯ --delete-orphaned-r2 ã‚’è¿½åŠ ã—ã¦ãã ã•ã„ã€‚")
        print("   python scripts/cleanup_duplicates.py --execute")
        print("=" * 60)
        return

    # å®Ÿè¡Œãƒ¢ãƒ¼ãƒ‰
    print("\nâ³ ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã‚’å®Ÿè¡Œä¸­...")

    # 4a: Notion ãƒšãƒ¼ã‚¸ã‚’ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–
    archived_count = 0
    archive_errors = 0
    for page in notion_pages_to_archive:
        page_id = page["id"]
        title = _get_page_title(page)
        try:
            notion.client.pages.update(page_id=page_id, archived=True)
            archived_count += 1
            logger.info(f"Archived Notion page: {title} ({page_id})")
        except Exception as e:
            archive_errors += 1
            logger.error(f"Failed to archive {title} ({page_id}): {e}")

    # 4b: R2 ç”»åƒã‚’å‰Šé™¤
    deleted_count = 0
    delete_errors = 0
    r2_keys_list = sorted(r2_keys_to_delete)
    # ãƒãƒƒãƒå‰Šé™¤ï¼ˆ1000ä»¶ãšã¤ï¼‰
    batch_size = 100
    for i in range(0, len(r2_keys_list), batch_size):
        batch = r2_keys_list[i : i + batch_size]
        for key in batch:
            try:
                r2.delete(key)
                deleted_count += 1
            except Exception as e:
                delete_errors += 1
                logger.error(f"Failed to delete R2 key {key}: {e}")

    # ------------------------------------------------
    # çµæœã‚µãƒãƒªãƒ¼
    # ------------------------------------------------
    print("\n" + "=" * 60)
    print("ğŸ‰ ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å®Œäº†")
    print("=" * 60)
    print(f"  Notionãƒšãƒ¼ã‚¸ ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–: {archived_count} ä»¶ (ã‚¨ãƒ©ãƒ¼: {archive_errors})")
    print(f"  R2ç”»åƒ å‰Šé™¤: {deleted_count} ä»¶ (ã‚¨ãƒ©ãƒ¼: {delete_errors})")
    print(f"  ä¿æŒ: æ•´å‚™æ¸ˆ {len(ready_pages)} ä»¶")


if __name__ == "__main__":
    main()
